{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 머신러닝 분류\n",
    "#### [1] 지도학습(Supervised Learning) : 답이 주어진 상태에서 학습\n",
    "* 회귀(Regression)\n",
    "* 분류(Classification)\n",
    "\n",
    "#### [2] 비지도학습(Unsupervised Learning) : 답을 모르고 학습\n",
    "* 군집화(Clustering)\n",
    "* 차원 축소(Dimension Reduction) : PCA(주성분 분석, Pricipal Component Analysis)\n",
    "\n",
    "#### [3] 강화 학습(Reinforcement Learning) : 답을 모르고 있는 상태에서 답을 알아가는 강한 인공지능(자아를 갖음, 인간수준)\n",
    "게임, 알파고(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론과 XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.5\n",
      "0\n",
      "0.5\n",
      "0\n",
      "1.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    print(tmp)\n",
    "    if tmp<=theta : # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta: \n",
    "        return 1\n",
    "\n",
    "print(AND(0,0))\n",
    "print(AND(0,1))\n",
    "print(AND(1,0))\n",
    "print(AND(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def NAND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.5\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp<=theta : # 임계값\n",
    "        return 1\n",
    "    elif tmp > theta: \n",
    "        return 0\n",
    "\n",
    "print(NAND(0,0))\n",
    "print(NAND(0,1))\n",
    "print(NAND(1,0))\n",
    "print(NAND(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def OR(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.4\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta :  # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(OR(0,0))    \n",
    "print(OR(0,1))\n",
    "print(OR(1,0))\n",
    "print(OR(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단층 퍼셉트론의 한계\n",
    "# XOR Problem : 아무리 학습시켜도 weight을 구할 수가 없음\n",
    "# def XOR(x1,x2):\n",
    "#     w1,w2,theta = 0.5,0.5,0.4\n",
    "#     tmp = w1*x1 + w2*x2\n",
    "#     print(tmp)\n",
    "#     if tmp<=theta : # 임계값\n",
    "#         return 0\n",
    "#     elif tmp > theta: \n",
    "#         return 1\n",
    "\n",
    "# print(XOR(0,0))\n",
    "# print(XOR(0,1))\n",
    "# print(XOR(1,0))\n",
    "# print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation : 1986년 제프리 힌튼(Geoffrey Hinton)\n",
    "샘플에 대한 신경망의 오차를 다시 출력층에서부터 입력층으로 거꾸로 전파시켜 각 층의 가중치(weight)를 계산하는 방법. 이를 통해 weight와 bias를 알맞게 학습할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층퍼셉트론(MLP)으로 XOR Problem 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0\n",
      "1.0\n",
      "1\n",
      "1.0\n",
      "1\n",
      "0.5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem : 서롣 다른 weight을 갖는 다층신경망을 사용하여 해결\n",
    "def XOR(x1,x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,0))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀(Regression)모델\n",
    "#### [1] 선형회귀(Linear Regression) :  1차 함수, 직선의 방정식\n",
    "\n",
    "#### [2] 가중치(Weight) : 입력변수가 출력에 영향을 미치는 정도를 설정, 기울기 값, 회귀 계수\n",
    "\n",
    "#### [3] 편향(Bias) : 기본 출력 값이 활성화 되는 정도를 설정, y절편, 회귀계수\n",
    "\n",
    "#### [4] 비용함수(Cost Function) : 2차 함수, 포물선의 방정식, (예측값-실제값)^2\n",
    "- cost(비용) = 오차 = 에러 = 손실(loss)\n",
    "- cost(W, b) = (H(x) - y)^2\n",
    "\n",
    "#### [5] 예측(가설,Hypothesis)함수 : predict, H(x) : 예측값, y값 : 답, 결정값, target, label, x: 입력, 피쳐(feature)\n",
    "- H(x) = W*X + b\n",
    "\n",
    "#### [6] 경사 하강법(Gradient Descent Algorithm)\n",
    "#### : 비용(cost) 이 가장 작은 Weight(가중치) 값을 구하는 알고리즘\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:-1,cost: 18.666666666666668\n",
      "w:0,cost: 4.666666666666667\n",
      "w:1,cost: 0.0\n",
      "w:2,cost: 4.666666666666667\n",
      "w:3,cost: 18.666666666666668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비용 함수의 구현\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]         # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2 # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)          # 평균 제곱 오차\n",
    "\n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "print('w:-1,cost:', cost(x,y,-1))  # hx = [-1,-2,-3], cost: 18.666666666666668\n",
    "print('w:0,cost:', cost(x,y,0))    # hx = [0,0,0]\n",
    "print('w:1,cost:', cost(x,y,1))    # hx = [1,2,3]  , w=1, cost=0.0, 최저점\n",
    "print('w:2,cost:', cost(x,y,2))    # hx = [2,4,6]\n",
    "print('w:3,cost:', cost(x,y,3))    # hx = [1,2,3],  cost: 18.666666666666668\n",
    "\n",
    "# 비용함수의 시각화 : x축은 weight, y축은 cost로 하는 2차 함수, 포물선의 방정식 \n",
    "for k in range(-30,50):\n",
    "    w = k/10\n",
    "    c = cost(x,y,w)\n",
    "    plt.plot(w,c,'ro')  # 'r':red, 'o': 점으로 출력\n",
    "    \n",
    "plt.title(' ** Cost Function Graph')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분 : 순간 변화량, 기울기, x축으로 1만큼 움직였을 때 y측으로 움직인 거리\n",
    "\n",
    "#### 함수의 미분 공식 정리 : f(x) = x^n ===> f '(x) = n*x^(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------start learning!!\n",
      "[000] cost: 378.0 old: 100 weight: 5.8\n",
      "[001] cost: 107.51999999999998 old: 378.0 weight: 3.56\n",
      "[002] cost: 30.583466666666666 old: 107.51999999999998 weight: 2.365333333333333\n",
      "[003] cost: 8.69929718518518 old: 30.583466666666666 weight: 1.7281777777777778\n",
      "[004] cost: 2.47446675489712 old: 8.69929718518518 weight: 1.3883614814814815\n",
      "[005] cost: 0.7038483213929583 old: 2.47446675489712 weight: 1.2071261234567903\n",
      "[006] cost: 0.2002057447517751 old: 0.7038483213929583 weight: 1.1104672658436214\n",
      "[007] cost: 0.05694741184050483 old: 0.2002057447517751 weight: 1.0589158751165981\n",
      "[008] cost: 0.016198374923521403 old: 0.05694741184050483 weight: 1.0314218000621858\n",
      "[009] cost: 0.004607537756023892 old: 0.016198374923521403 weight: 1.0167582933664991\n",
      "[010] cost: 0.0013105885172690224 old: 0.004607537756023892 weight: 1.008937756462133\n",
      "[011] cost: 0.0003727896226898598 old: 0.0013105885172690224 weight: 1.004766803446471\n",
      "[012] cost: 0.0001060379371206724 old: 0.0003727896226898598 weight: 1.002542295171451\n",
      "[013] cost: 3.0161902114324568e-05 old: 0.0001060379371206724 weight: 1.0013558907581073\n",
      "[014] cost: 8.579385490296031e-06 old: 3.0161902114324568e-05 weight: 1.0007231417376572\n",
      "[015] cost: 2.4403585394623746e-06 old: 8.579385490296031e-06 weight: 1.000385675593417\n",
      "[016] cost: 6.941464290019953e-07 old: 2.4403585394623746e-06 weight: 1.0002056936498225\n",
      "[017] cost: 1.9744609536079287e-07 old: 6.941464290019953e-07 weight: 1.0001097032799053\n",
      "[018] cost: 5.616244490256599e-08 old: 1.9744609536079287e-07 weight: 1.0000585084159495\n",
      "[019] cost: 1.597509543893363e-08 old: 5.616244490256599e-08 weight: 1.0000312044885065\n",
      "[020] cost: 4.544027147100321e-09 old: 1.597509543893363e-08 weight: 1.0000166423938701\n",
      "[021] cost: 1.292523277397425e-09 old: 4.544027147100321e-09 weight: 1.0000088759433974\n",
      "[022] cost: 3.6765106557353845e-10 old: 1.292523277397425e-09 weight: 1.0000047338364786\n",
      "[023] cost: 1.0457630309680019e-10 old: 3.6765106557353845e-10 weight: 1.0000025247127886\n",
      "[024] cost: 2.974614843652282e-11 old: 1.0457630309680019e-10 weight: 1.0000013465134872\n",
      "[025] cost: 8.461126664488492e-12 old: 2.974614843652282e-11 weight: 1.0000007181405266\n",
      "[026] cost: 2.4067204744183466e-12 old: 8.461126664488492e-12 weight: 1.0000003830082809\n",
      "[027] cost: 6.845782681618235e-13 old: 2.4067204744183466e-12 weight: 1.0000002042710832\n",
      "[028] cost: 1.9472448527085553e-13 old: 6.845782681618235e-13 weight: 1.0000001089445776\n",
      "[029] cost: 5.538829787885448e-14 old: 1.9472448527085553e-13 weight: 1.0000000581037747\n",
      "[030] cost: 1.575489361658054e-14 old: 5.538829787885448e-14 weight: 1.0000000309886798\n",
      "[031] cost: 4.481391952569267e-15 old: 1.575489361658054e-14 weight: 1.000000016527296\n",
      "[032] cost: 1.2747070573345361e-15 old: 4.481391952569267e-15 weight: 1.0000000088145578\n",
      "[033] cost: 3.6258332831368247e-16 old: 1.2747070573345361e-15 weight: 1.0000000047010975\n",
      "[034] cost: 1.0313481853667948e-16 old: 3.6258332831368247e-16 weight: 1.000000002507252\n",
      "[035] cost: 2.933612341505031e-17 old: 1.0313481853667948e-16 weight: 1.000000001337201\n",
      "[036] cost: 8.344497274162171e-18 old: 2.933612341505031e-17 weight: 1.000000000713174\n",
      "[037] cost: 2.3735464825145457e-18 old: 8.344497274162171e-18 weight: 1.0000000003803595\n",
      "[038] cost: 6.751422044227725e-19 old: 2.3735464825145457e-18 weight: 1.0000000002028584\n",
      "[039] cost: 1.920405593646558e-19 old: 6.751422044227725e-19 weight: 1.0000000001081912\n",
      "[040] cost: 5.462493428133877e-20 old: 1.920405593646558e-19 weight: 1.000000000057702\n",
      "[041] cost: 1.5537773320496455e-20 old: 5.462493428133877e-20 weight: 1.0000000000307745\n",
      "[042] cost: 4.419657596231936e-21 old: 1.5537773320496455e-20 weight: 1.000000000016413\n",
      "[043] cost: 1.2571515848923574e-21 old: 4.419657596231936e-21 weight: 1.0000000000087537\n",
      "[044] cost: 3.5758710617043957e-22 old: 1.2571515848923574e-21 weight: 1.0000000000046687\n",
      "[045] cost: 1.0171864166944069e-22 old: 3.5758710617043957e-22 weight: 1.00000000000249\n",
      "[046] cost: 2.8933990559698716e-23 old: 1.0171864166944069e-22 weight: 1.000000000001328\n",
      "[047] cost: 8.231253103671774e-24 old: 2.8933990559698716e-23 weight: 1.0000000000007083\n",
      "[048] cost: 2.341362175139032e-24 old: 8.231253103671774e-24 weight: 1.0000000000003777\n",
      "[049] cost: 6.658942698258942e-25 old: 2.341362175139032e-24 weight: 1.0000000000002014\n",
      "[050] cost: 1.891893061517278e-25 old: 6.658942698258942e-25 weight: 1.0000000000001075\n",
      "[051] cost: 5.389865839559056e-26 old: 1.891893061517278e-25 weight: 1.0000000000000573\n",
      "[052] cost: 1.5315340044413334e-26 old: 5.389865839559056e-26 weight: 1.0000000000000306\n",
      "[053] cost: 4.38172789805011e-27 old: 1.5315340044413334e-26 weight: 1.0000000000000164\n",
      "[054] cost: 1.2599423424554927e-27 old: 4.38172789805011e-27 weight: 1.0000000000000087\n",
      "[055] cost: 3.46129156767911e-28 old: 1.2599423424554927e-27 weight: 1.0000000000000047\n",
      "[056] cost: 1.0355442841244991e-28 old: 3.46129156767911e-28 weight: 1.0000000000000024\n",
      "[057] cost: 2.677196697093809e-29 old: 1.0355442841244991e-28 weight: 1.0000000000000013\n",
      "[058] cost: 8.283039504820624e-30 old: 2.677196697093809e-29 weight: 1.0000000000000007\n",
      "[059] cost: 1.791371638939381e-30 old: 8.283039504820624e-30 weight: 1.0000000000000004\n",
      "[060] cost: 9.203377227578472e-31 old: 1.791371638939381e-30 weight: 1.0000000000000002\n",
      "[061] cost: 3.4512664603419266e-31 old: 9.203377227578472e-31 weight: 1.0\n",
      "[062] cost: 0.0 old: 3.4512664603419266e-31 weight: 1.0\n",
      "[063] cost: 0.0 old: 0.0 weight: 1.0\n",
      "-------------end learning!!\n"
     ]
    }
   ],
   "source": [
    "# 경사 하강법 알고리즘 함수 구현, 미분 적용\n",
    "def gradient_descent(x,y,w):\n",
    "    c=0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx-y[k])*x[k]     # 곱하기2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "    return c/len(x)\n",
    "\n",
    "# x=[1,2,3]\n",
    "# y=[1,2,3,4,5]\n",
    "\n",
    "# 학습 시작(train,fit) 시작\n",
    "print('------------start learning!!')\n",
    "w, old = 10,100\n",
    "for k in range(1000):\n",
    "    c=cost(x,y,w)\n",
    "    grad = gradient_descent(x,y,w)\n",
    "    w -= 0.1*grad   # 0.1 : 학습율(learning rate), 하이퍼 파라메터, 가중치의 업데이트실행\n",
    "    print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "#   if c == old:  # cost의 변화가 없을 때\n",
    "    if c >=old and abs(c-old) < 1.0e-15:  # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "        break\n",
    "    old = c\n",
    "    \n",
    "print('-------------end learning!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear Regression\n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "\n",
    "# (1) 비용함수 구현\n",
    "def cost(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]         # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2 # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)          # 평균 제곱 오차\n",
    "\n",
    "# (2) 경사 하강법 알고리즘 함수 구현\n",
    "def gradient_descent(x,y,w):\n",
    "    c=0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx-y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 w에 대한 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "    return c/len(x)\n",
    "\n",
    "# (3) 학습(fit) 함수 구현\n",
    "def fit(x,y):\n",
    "    print('------------start learning!!')\n",
    "    w, old = 10,100\n",
    "    for k in range(1000):\n",
    "        c=cost(x,y,w)\n",
    "        grad = gradient_descent(x,y,w)\n",
    "        w -= 0.1*grad   # 0.1 : 학습율(learning rate), 하이퍼 파라메터, 가중치의 업데이트실행\n",
    "        print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "    #   if c == old:  # cost의 변화가 없을 때\n",
    "        if c >=old and abs(c-old) < 1.0e-15:  # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "            break\n",
    "        old = c\n",
    "    print('-------------end learning!!')\n",
    "    return w\n",
    "    \n",
    "# (4) 예측(predict) 함수 구현\n",
    "def predict(x,w):\n",
    "    hx = w*np.array(x)\n",
    "    return list(hx)\n",
    "\n",
    "# (5) 정확도(평가지표) 측정 함수 구현 : 정확도 검증(Validation)\n",
    "# <1> 분류(classification) 일 때 : 정확도(%)\n",
    "def get_accuaracy(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    correct = 0\n",
    "    for k, _ in enumerate(y_test):\n",
    "        if y_test[k] == y_pred[k]:  # 실제값과 예측값이 같으면 correct를 1증가\n",
    "            correct +=1\n",
    "    accuracy = round(correct/len(y_test),2)  # 맞은 갯수/전체 갯수  # 2자리로 반올림\n",
    "    return accuracy\n",
    "\n",
    "# <2> 회귀 모델(Linear Regression) 일 때 : RMSE(Root Mean Squared Error, 평균 제곱근 오차)\n",
    "def get_rmse(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (y_pred[k] - y_test[k])**2  # 오차의 제곱을 합한다.\n",
    "    mse = squared_error/len(y_test)   # 오차의 제곱의 평균, 평균 제곱 오차\n",
    "    rmse = np.sqrt(mse)               # 제곱근(Root), 평균 제곱근 오차\n",
    "    return rmse            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------start learning!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "-------------end learning!!\n",
      "weight: 2.0\n",
      "y_pred [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuracy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "w = fit(x_train, y_train)\n",
    "print('weight:',w)  # w:2.0 , cost:0.0\n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]  # x 값만 사용\n",
    "y_pred = predict(x_pred,w)\n",
    "print('y_pred',y_pred)   # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = get_accuaracy(x_test,y_test,w)\n",
    "print('Accuracy:',accuracy)  # 0.67\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = get_rmse(x_test,y_test,w)\n",
    "print('RMSE:',rmse)   # RMSE: 5.773502691896258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### class를 사용한 Linear Regression 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self): # 생성자\n",
    "        self.w = 0\n",
    "        print('LinearRegression 생성자')\n",
    "        \n",
    "    # (1) 비용함수 구현\n",
    "    def cost(self,x,y) :\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w * x[k]     # 예측 함수(방정식)\n",
    "            loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "            c += loss\n",
    "        return c/len(x)            #  평균 제곱 오차\n",
    "\n",
    "    # (2) 경사 하강법 알고리즘 함수 구현\n",
    "    def gradient_descent(self,x,y):\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w*x[k] \n",
    "            loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "            c += loss\n",
    "            # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "            # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "            # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "            # =  2*x[k]*(hx - y[k])\n",
    "        return c/len(x)\n",
    "\n",
    "    # (3) 학습(fit) 함수 구현\n",
    "    def fit(self,x,y):\n",
    "        print('----------- start learning!!')\n",
    "        self.w , old = 10, 100\n",
    "        for k in range(1000):\n",
    "            c = self.cost(x,y)\n",
    "            grad = self.gradient_descent(x,y)\n",
    "            self.w -= 0.1*grad  # 0.1:학습율(learning rate),하이퍼 파라메터,가중치의 업데이트실행\n",
    "            print('[%03d]'%k,'cost:',c,'old:',old,'weight:',self.w)\n",
    "        #   if c == old:  # cost의 변화가 없을 때\n",
    "            if c >= old and abs(c - old) < 1.0e-15: # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "                break\n",
    "            old = c\n",
    "        print('----------- end learning!!')  \n",
    "#         return self.w\n",
    "\n",
    "    # (4) 예측(predict) 함수 구현\n",
    "    def predict(self,x):\n",
    "        hx = self.w*np.array(x)\n",
    "        return list(hx)\n",
    "\n",
    "    # (5) 정확도(평가지표) 측정 함수 구현 :  정확도 검증(Valiation)  \n",
    "    # <1> 분류(classification) 일 때 : 정확도(%)\n",
    "    def get_accuaracy(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        correct = 0\n",
    "        for k,_ in enumerate(y_test) :\n",
    "            if y_test[k] == y_pred[k] :  # 실제값과 예측값이 같으면 correct를 1증가\n",
    "                correct += 1\n",
    "        accuracy = round(correct/len(y_test),2)  # 맞은 갯수/전체 갯수\n",
    "        return accuracy                \n",
    "\n",
    "    # <2> 회귀(Linear Regression) 일 때 : RMSE(Root Mean Squared Error,평균 제곱근 오차)\n",
    "    def get_rmse(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        squared_error = 0\n",
    "        for k,_ in enumerate(y_test):\n",
    "            squared_error += (y_pred[k] - y_test[k])**2  # 오차의 제곱을 합한다\n",
    "        mse = squared_error/len(y_test)   # 오차의 제곱의 평균, 평균 제곱 오차\n",
    "        rmse = np.sqrt(mse)               # 제곱근(Root), 평균 제곱근 오차\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------start learning!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "-------------end learning!!\n",
      "weight: 2.0\n",
      "y_pred [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuracy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현, class의 인스턴스를 생성하여 사용\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "w = fit(x_train, y_train)\n",
    "print('weight:',w)  # w:2.0 , cost:0.0\n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]  # x 값만 사용\n",
    "y_pred = predict(x_pred,w)\n",
    "print('y_pred',y_pred)   # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = get_accuaracy(x_test,y_test,w)\n",
    "print('Accuracy:',accuracy)  # 0.67\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = get_rmse(x_test,y_test,w)\n",
    "print('RMSE:',rmse)   # RMSE: 5.773502691896258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 생성자\n",
      "----------- start learning!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "----------- end learning!!\n",
      "weight: 2.0\n",
      "y_pred: [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuarcy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현 , class의 인스턴스를 생성하여 사용\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습 수행\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "lr = LinearRegression()  # class의 인스턴스 객체를 생성\n",
    "lr.fit(x_train,y_train)  # 인스턴스 메서드를 호출\n",
    "print('weight:',lr.w)    # w:2.0 , cost:0.0\n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]   # x 값만 사용\n",
    "y_pred = lr.predict(x_pred)  # 인스턴스 메서드를 호출\n",
    "print('y_pred:',y_pred)  # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = lr.get_accuaracy(x_test,y_test)  # 인스턴스 메서드를 호출\n",
    "print('Accuarcy:',accuracy)  # 0.67  , 절대 지표\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = lr.get_rmse(x_test,y_test)  # 인스턴스 메서드를 호출\n",
    "print('RMSE:',rmse) # RMSE: 5.773502691896258 , 상대 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
